{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedc0066",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d338ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretisation method: equal_width. Thresholds = {}\n",
      "Test accuracy: 95.56%\n",
      "=== Learned tree ===\n",
      "Feature: petalwidth\n",
      " ├── [0]\n",
      " │   Label: Iris-setosa\n",
      " ├── [1]\n",
      " │   Label: Iris-versicolor\n",
      " ├── [2]\n",
      " │   Feature: petallength\n",
      " │    ├── [1]\n",
      " │    │   Label: Iris-versicolor\n",
      " │    ├── [2]\n",
      " │    │   Label: Iris-versicolor\n",
      " │    ├── [3]\n",
      " │    │   Label: Iris-virginica\n",
      " ├── [3]\n",
      " │   Label: Iris-virginica\n",
      "Discretisation method: entropy. Thresholds = {'sepallength': 5.55, 'sepalwidth': 3.3499999999999996, 'petallength': 2.45, 'petalwidth': 0.8}\n",
      "Test accuracy: 68.89%\n",
      "=== Learned tree ===\n",
      "Feature: petallength\n",
      " ├── [0]\n",
      " │   Feature: sepallength\n",
      " │    ├── [0]\n",
      " │    │   Label: Iris-virginica\n",
      " │    ├── [1]\n",
      " │    │   Label: Iris-versicolor\n",
      " ├── [1]\n",
      " │   Label: Iris-setosa\n",
      "Discretisation method: cart. Thresholds = {}\n",
      "Test accuracy: 95.56%\n",
      "=== Learned tree ===\n",
      "Feature: petallength <= 2.450 ?\n",
      " ├── True:\n",
      " │   Label: Iris-setosa\n",
      " └── False:\n",
      "     Feature: petalwidth <= 1.650 ?\n",
      "      ├── True:\n",
      "      │   Label: Iris-versicolor\n",
      "      └── False:\n",
      "          Label: Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import Dict, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0.  Generic helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _entropy(labels) -> float:\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    return -sum((c / total) * math.log2(c / total) for c in counts.values())\n",
    "\n",
    "\n",
    "def _gini(labels) -> float:\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    return 1.0 - sum((c / total) ** 2 for c in counts.values())\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  Numeric-feature threshold search & discretisers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _best_threshold(values: np.ndarray, labels: np.ndarray, *, criterion: str = \"entropy\") -> Tuple[Optional[float], float]:\n",
    "    order = np.argsort(values)\n",
    "    v_sorted, y_sorted = values[order], labels[order]\n",
    "    uniques = np.unique(v_sorted)\n",
    "    if len(uniques) == 1:\n",
    "        return None, 0.0\n",
    "\n",
    "    impurity = _entropy if criterion == \"entropy\" else _gini\n",
    "    total_imp = impurity(y_sorted)\n",
    "\n",
    "    best_gain, best_thr = 0.0, None\n",
    "    n = len(values)\n",
    "    for i in range(1, n):\n",
    "        if v_sorted[i] == v_sorted[i - 1]:\n",
    "            continue\n",
    "        thr = (v_sorted[i] + v_sorted[i - 1]) / 2\n",
    "        left_y, right_y = y_sorted[:i], y_sorted[i:]\n",
    "        gain = total_imp - (\n",
    "            (len(left_y) / n) * impurity(left_y) + (len(right_y) / n) * impurity(right_y)\n",
    "        )\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_thr = gain, thr\n",
    "    return best_thr, best_gain\n",
    "\n",
    "\n",
    "def discretise_equal_width(series: pd.Series, bins: int = 4) -> pd.Series:\n",
    "    return pd.cut(series, bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "\n",
    "def discretise_entropy(series: pd.Series, labels: pd.Series):\n",
    "    thr, _ = _best_threshold(series.to_numpy(), labels.to_numpy(), criterion=\"entropy\")\n",
    "    if thr is None:\n",
    "        return pd.Series(np.zeros(len(series), dtype=int), index=series.index), None\n",
    "    return (series <= thr).astype(int), thr\n",
    "\n",
    "\n",
    "def apply_binary_entropy(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    Xd, thresholds = {}, {}\n",
    "    for col in X.columns:\n",
    "        if np.issubdtype(X[col].dtype, np.number):\n",
    "            Xd[col], thr = discretise_entropy(X[col], y)\n",
    "            if thr is not None:\n",
    "                thresholds[col] = thr\n",
    "        else:\n",
    "            Xd[col] = X[col]\n",
    "    return pd.DataFrame(Xd), thresholds\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Shared node dataclass\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    prediction: Any\n",
    "    feature: Optional[str] = None\n",
    "    threshold: Optional[float] = None\n",
    "    children: Dict[Any, \"Node\"] | None = None\n",
    "    left: \"Node\" | None = None\n",
    "    right: \"Node\" | None = None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.feature is None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.  ID3 implementation (categorical-only)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    def print_tree(self, node: Optional[Node] = None, indent: str = \"\", depth: int = 0, max_depth_print: int = 2):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.is_leaf() or depth >= max_depth_print:\n",
    "            print(indent + f\"Label: {node.prediction}\")\n",
    "            return\n",
    "        print(indent + f\"Feature: {node.feature}\")\n",
    "        for val, child in node.children.items():\n",
    "            print(indent + f\" ├── [{val}]\")\n",
    "            self.print_tree(child, indent + \" │   \", depth + 1, max_depth_print)\n",
    "\n",
    "    def __init__(self, max_depth: Optional[int] = None):\n",
    "        self.root: Node | None = None\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _info_gain(self, data: pd.DataFrame, feature: str) -> float:\n",
    "        total = _entropy(data[\"label\"])\n",
    "        weighted = sum(\n",
    "            len(sub) / len(data) * _entropy(sub[\"label\"]) for _, sub in data.groupby(feature)\n",
    "        )\n",
    "        return total - weighted\n",
    "\n",
    "    def _build(self, data: pd.DataFrame, features: pd.Index, depth: int) -> Node:\n",
    "        y = data[\"label\"]\n",
    "        if len(y.unique()) == 1:\n",
    "            return Node(prediction=y.iloc[0])\n",
    "        if not len(features) or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return Node(prediction=y.mode()[0])\n",
    "\n",
    "        best_feat = max(features, key=lambda f: self._info_gain(data, f))\n",
    "        node = Node(prediction=y.mode()[0], feature=best_feat, children={})\n",
    "        for val, sub in data.groupby(best_feat):\n",
    "            node.children[val] = self._build(sub.drop(columns=[best_feat]), features.drop(best_feat), depth + 1)\n",
    "        return node\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        data = X.copy()\n",
    "        data[\"label\"] = y\n",
    "        self.root = self._build(data, X.columns, 0)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return X.apply(lambda r: self._predict_row(r, self.root), axis=1)\n",
    "\n",
    "    def _predict_row(self, row, node):\n",
    "        while not node.is_leaf():\n",
    "            node = node.children.get(row[node.feature], node)\n",
    "        return node.prediction\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  CART implementation (numeric & categorical)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class CARTDecisionTree:\n",
    "    def __init__(self, *, max_depth: Optional[int] = None, min_samples_split: int = 2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root: Node | None = None\n",
    "\n",
    "    def print_tree(self, node: Optional[Node] = None, indent: str = \"\", depth: int = 0, max_depth_print: int = 2):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.is_leaf() or depth >= max_depth_print:\n",
    "            print(indent + f\"Label: {node.prediction}\")\n",
    "            return\n",
    "        # print numeric split\n",
    "        print(indent + f\"Feature: {node.feature} <= {node.threshold:.3f} ?\")\n",
    "        print(indent + \" ├── True:\")\n",
    "        self.print_tree(node.left, indent + \" │   \", depth+1, max_depth_print)\n",
    "        print(indent + \" └── False:\")\n",
    "        self.print_tree(node.right, indent + \"     \", depth+1, max_depth_print)\n",
    "\n",
    "    def _best_split(self, X: pd.DataFrame, y: pd.Series):\n",
    "        best_gain, best_feat, best_thr, best_masks = 0.0, None, None, None\n",
    "        base_imp = _gini(y)\n",
    "        for feature in X.columns:\n",
    "            col = X[feature]\n",
    "            if np.issubdtype(col.dtype, np.number):\n",
    "                thr, gain = _best_threshold(col.to_numpy(), y.to_numpy(), criterion=\"gini\")\n",
    "                if thr is None or gain <= best_gain:\n",
    "                    continue\n",
    "                mask = col <= thr\n",
    "                best_gain, best_feat, best_thr, best_masks = gain, feature, thr, (mask, ~mask)\n",
    "            else:\n",
    "                for val in col.unique():\n",
    "                    mask = col == val\n",
    "                    gain = base_imp - (mask.mean() * _gini(y[mask]) + (~mask).mean() * _gini(y[~mask]))\n",
    "                    if gain > best_gain:\n",
    "                        best_gain, best_feat, best_thr, best_masks = gain, feature, val, (mask, ~mask)\n",
    "        return best_feat, best_thr, best_masks, best_gain\n",
    "\n",
    "    def _build(self, X: pd.DataFrame, y: pd.Series, depth: int) -> Node:\n",
    "        if (\n",
    "            len(y.unique()) == 1\n",
    "            or len(X) < self.min_samples_split\n",
    "            or (self.max_depth is not None and depth >= self.max_depth)\n",
    "        ):\n",
    "            return Node(prediction=y.mode()[0])\n",
    "        feat, thr, masks, gain = self._best_split(X, y)\n",
    "        if feat is None or gain == 0:\n",
    "            return Node(prediction=y.mode()[0])\n",
    "        node = Node(prediction=y.mode()[0], feature=feat, threshold=thr)\n",
    "        left_mask, right_mask = masks\n",
    "        node.left = self._build(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self._build(X[right_mask], y[right_mask], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        self.root = self._build(X.reset_index(drop=True), y.reset_index(drop=True), 0)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return X.apply(lambda r: self._predict_row(r, self.root), axis=1)\n",
    "\n",
    "    def _predict_row(self, row, node):\n",
    "        while not node.is_leaf():\n",
    "            if node.threshold is not None and np.issubdtype(type(row[node.feature]), np.number):\n",
    "                node = node.left if row[node.feature] <= node.threshold else node.right\n",
    "            else:\n",
    "                node = node.left if row[node.feature] == node.threshold else node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series):\n",
    "        return (X.apply(lambda r: self._predict_row(r, self.root), axis=1) == y).mean()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.  Simple train/test split helper (no sklearn)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def simple_train_test_split(X: pd.DataFrame, y: pd.Series, *, test_ratio: float = 0.3, seed: int = 0):\n",
    "    idx = X.sample(frac=1.0, random_state=seed).index\n",
    "    cut = int(len(idx) * (1 - test_ratio))\n",
    "    return X.loc[idx[:cut]], X.loc[idx[cut:]], y.loc[idx[:cut]], y.loc[idx[cut:]]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6.  Demo & sanity check\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iris = pd.read_csv(\"iris.csv\")\n",
    "    X_raw = iris.iloc[:, 1:-1]\n",
    "    y = iris.iloc[:, -1]\n",
    "\n",
    "    def run_demo(method: str):\n",
    "        if method == \"equal_width\":\n",
    "            X_disc = X_raw.apply(discretise_equal_width)\n",
    "            tree = ID3DecisionTree()\n",
    "            X_tr, X_te, y_tr, y_te = simple_train_test_split(X_disc, y)\n",
    "            tree.fit(X_tr, y_tr)\n",
    "            acc = (tree.predict(X_te) == y_te).mean() * 100\n",
    "            print(f\"Discretisation method: {method}. Thresholds = {{}}\")\n",
    "            print(f\"Test accuracy: {acc:.2f}%\")\n",
    "            print(\"=== Learned tree ===\")\n",
    "            tree.print_tree(max_depth_print=2)\n",
    "\n",
    "        elif method == \"entropy\":\n",
    "            X_disc, thresholds = apply_binary_entropy(X_raw, y)\n",
    "            tree = ID3DecisionTree()\n",
    "            X_tr, X_te, y_tr, y_te = simple_train_test_split(X_disc, y)\n",
    "            tree.fit(X_tr, y_tr)\n",
    "            acc = (tree.predict(X_te) == y_te).mean() * 100\n",
    "            print(f\"Discretisation method: {method}. Thresholds = {thresholds}\")\n",
    "            print(f\"Test accuracy: {acc:.2f}%\")\n",
    "            print(\"=== Learned tree ===\")\n",
    "            tree.print_tree(max_depth_print=2)\n",
    "\n",
    "        elif method == \"cart\":\n",
    "\n",
    "            X_tr, X_te, y_tr, y_te = simple_train_test_split(X_raw, y)\n",
    "            tree = CARTDecisionTree()\n",
    "            tree.fit(X_tr, y_tr)\n",
    "\n",
    "            acc = tree.score(X_te, y_te) * 100\n",
    "            print(f\"Discretisation method: {method}. Thresholds = {{}}\")\n",
    "            print(f\"Test accuracy: {acc:.2f}%\")\n",
    "            print(\"=== Learned tree ===\")\n",
    "            tree.print_tree(max_depth_print=2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method: {method}\")\n",
    "\n",
    "    # uso\n",
    "    run_demo(\"equal_width\")\n",
    "    run_demo(\"entropy\")\n",
    "    run_demo(\"cart\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5c933",
   "metadata": {},
   "source": [
    "Equal-width bins (quartiles) kept 4 groups per measurement, so the tree had enough detail to tell the three iris species apart.\n",
    "\n",
    "Entropy forced each measurement into just 2 groups (“small” or “large”).\n",
    "With only two options, the tree lost the extra cut-point it needs to separate versicolor from virginica, so accuracy dropped.\n",
    "\n",
    "Unlike binary discretisation, CART learns optimal split thresholds per feature at each node allowing multiple cut-points and preserving the granularity needed to separate similar classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
